{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T03:38:28.018774Z",
     "iopub.status.busy": "2024-11-18T03:38:28.018462Z",
     "iopub.status.idle": "2024-11-18T03:38:32.073006Z",
     "shell.execute_reply": "2024-11-18T03:38:32.072248Z"
    }
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout, Flatten, Dense, Conv2DTranspose\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T03:38:32.075068Z",
     "iopub.status.busy": "2024-11-18T03:38:32.074676Z",
     "iopub.status.idle": "2024-11-18T03:38:32.078229Z",
     "shell.execute_reply": "2024-11-18T03:38:32.077628Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 128 \n",
    "IMAGE_SIZE = 224  # Increased image size to improve feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T03:38:32.079831Z",
     "iopub.status.busy": "2024-11-18T03:38:32.079589Z",
     "iopub.status.idle": "2024-11-18T03:38:32.082711Z",
     "shell.execute_reply": "2024-11-18T03:38:32.082124Z"
    }
   },
   "outputs": [],
   "source": [
    "training_dataset_path = os.path.join(os.environ['DATASET_PATH'], 'Training')\n",
    "\n",
    "class_labels = {'glioma': 0, 'meningioma': 1, 'pituitary': 2, 'notumor': 3}\n",
    "image_folders = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
    "class_labels_reverse = {v: k for k, v in class_labels.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T03:38:32.084219Z",
     "iopub.status.busy": "2024-11-18T03:38:32.084019Z",
     "iopub.status.idle": "2024-11-18T03:38:45.070092Z",
     "shell.execute_reply": "2024-11-18T03:38:45.069347Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize lists to store images and masks for the training dataset\n",
    "images = []\n",
    "masks = []\n",
    "\n",
    "# Loop over the folders in the Training dataset to load images and their corresponding masks\n",
    "for folder in image_folders:\n",
    "    folder_path = os.path.join(training_dataset_path, folder)\n",
    "    \n",
    "    # List image files in the folder\n",
    "    image_files = sorted(os.listdir(folder_path))\n",
    "\n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(folder_path, img_file)\n",
    "\n",
    "        # Read the image (grayscale because MRI data)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Resize image if necessary (standardize image size)\n",
    "        img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))  # Resize to 224x224 or any size you prefer\n",
    "\n",
    "        # Create a dummy mask for segmentation (since the dataset is for classification, we generate synthetic masks for demonstration purposes)\n",
    "        mask = np.zeros((IMAGE_SIZE, IMAGE_SIZE), dtype=np.uint8)\n",
    "        if folder == 'glioma':\n",
    "            mask[50:150, 50:150] = 1  # Example synthetic mask for glioma\n",
    "        elif folder == 'meningioma':\n",
    "            mask[70:170, 70:170] = 1  # Example synthetic mask for meningioma\n",
    "        elif folder == 'pituitary':\n",
    "            mask[30:130, 30:130] = 1  # Example synthetic mask for pituitary\n",
    "        elif folder == 'notumor':\n",
    "            mask[:, :] = 0  # No mask for 'notumor'\n",
    "\n",
    "        # Append image and mask (use list append method)\n",
    "        images.append(img)\n",
    "        masks.append(mask)\n",
    "\n",
    "# Convert lists to numpy arrays after appending all images and masks\n",
    "images = np.array(images)\n",
    "masks = np.array(masks)\n",
    "\n",
    "# Normalize the images (optional but helps with training)\n",
    "images = images.astype('float32') / 255.0\n",
    "\n",
    "# Convert images to 3 channels (grayscale to RGB) for segmentation model\n",
    "X_rgb = np.repeat(images.reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 1), 3, axis=-1)\n",
    "\n",
    "# Normalize the masks\n",
    "masks = masks.astype('float32')\n",
    "\n",
    "# Expand mask dimensions to match output (batch, height, width, channels)\n",
    "masks = np.expand_dims(masks, axis=-1)\n",
    "\n",
    "# Split the dataset into training and validation sets (80% for training, 20% for validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_rgb, masks, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T03:38:45.073073Z",
     "iopub.status.busy": "2024-11-18T03:38:45.072779Z",
     "iopub.status.idle": "2024-11-18T03:38:46.344774Z",
     "shell.execute_reply": "2024-11-18T03:38:46.344231Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the U-Net model for segmentation\n",
    "def unet_model(input_size=(IMAGE_SIZE, IMAGE_SIZE, 3)):\n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    # Encoder\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
    "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "\n",
    "    # Bottleneck\n",
    "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
    "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "    # Decoder\n",
    "    u6 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
    "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
    "\n",
    "    u7 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
    "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
    "\n",
    "    u8 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
    "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
    "\n",
    "    u9 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
    "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the U-Net model\n",
    "model = unet_model()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T03:38:46.346194Z",
     "iopub.status.busy": "2024-11-18T03:38:46.346004Z",
     "iopub.status.idle": "2024-11-18T04:17:36.288524Z",
     "shell.execute_reply": "2024-11-18T04:17:36.287856Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model using the training data\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model Output\n",
    "model.save('segmentation1_model.h5')\n",
    "\n",
    "# Save the model history to a file\n",
    "with open('segmentation1_training_history.pkl', 'wb') as file:\n",
    "    pickle.dump(history.history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T04:17:36.291637Z",
     "iopub.status.busy": "2024-11-18T04:17:36.291192Z",
     "iopub.status.idle": "2024-11-18T04:17:40.961623Z",
     "shell.execute_reply": "2024-11-18T04:17:40.961060Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training Graph: Accuracy and Loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy / Loss')\n",
    "plt.title('Training Performance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Validation Graph: Accuracy and Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy / Loss')\n",
    "plt.title('Validation Performance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the validation (split) set\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T04:17:40.963229Z",
     "iopub.status.busy": "2024-11-18T04:17:40.962951Z",
     "iopub.status.idle": "2024-11-18T04:17:48.441721Z",
     "shell.execute_reply": "2024-11-18T04:17:48.441194Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test the model on a single image\n",
    "def test_segmentation(model, image_path):\n",
    "    # Load and preprocess the input image\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))  # Resize to match training input size\n",
    "    img = img.astype('float32') / 255.0  # Normalize\n",
    "    img = np.repeat(img, 3, axis=-1)  # Convert grayscale to RGB\n",
    "    img = img.reshape(1, IMAGE_SIZE, IMAGE_SIZE, 3)  # Add batch dimension\n",
    "    \n",
    "    # Predict the mask\n",
    "    prediction = model.predict(img)\n",
    "    predicted_mask = (prediction > 0.5).astype(np.uint8)  # Threshold the output to get binary mask\n",
    "\n",
    "    # Plot the input image and predicted mask\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img[0, :, :, 0], cmap='gray')\n",
    "    plt.title('Input Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(predicted_mask[0, :, :, 0], cmap='gray')\n",
    "    plt.title('Predicted Mask')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Test the model on a sample image\n",
    "test_image_path = os.path.join(os.environ['DATASET_PATH'], 'Testing/glioma/Te-gl_0010.jpg')\n",
    "test_segmentation(model, test_image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CPS843-Brain-Tumor-Detection-NzvbpuGq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
